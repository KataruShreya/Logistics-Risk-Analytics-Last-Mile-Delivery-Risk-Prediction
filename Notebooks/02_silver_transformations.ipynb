{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18dcdaad-bbdb-4d33-8c9d-e48d4bd0e098",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "I started by reading the Bronze Delta table into a Spark DataFrame. This serves as the raw-but-ingested source for the Silver layer, and I also validated row counts before applying any transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff23daf2-1ef2-42b7-8ce8-d655cb1ac833",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "df_bronze = spark.table(\"capstone_project.logistics.bronze_last_mile_deliveries\")\n",
    "print(\"Initial Bronze Count:\", df_bronze.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a1b00f3-fb1e-4dce-b190-be34b4749523",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Here, I checked the schema and total record count of the Bronze data. This gives me a baseline so I can later confirm how many records are affected by cleaning and transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35a25ff9-929a-4670-b891-fff04cde58be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_bronze.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6051c2d-369e-4e98-bff3-a77cfa1e7b0a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_bronze.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7828b5bf-f3db-4c4b-a465-e2caa593b06f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "display(df_bronze.limit(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4556910a-4068-4cef-b99d-eb1d3c4adb7a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "I displayed the Bronze data and listed the column names to get a quick sense of the data structure and verify that all expected fields are present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d8393ef-2233-49ed-963e-d681a2a8020f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_bronze.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8b52b58-7a85-4f76-adc3-25b4184c7e88",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "In this step, I removed duplicate records and dropped rows with missing values in critical operational fields such as order ID, courier ID, city, and key timestamps. These fields are essential for reliable downstream calculations.\n",
    "\n",
    "After removing duplicates and nulls from key columns, I checked the row count again to understand how much data was filtered out during this initial cleanup step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e6af7e1-6c8f-4f90-9b16-fe18cde07bb7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_silver = (\n",
    "    df_bronze\n",
    "    .dropDuplicates()\n",
    "    .dropna(subset=[ \n",
    "        \"order_id\",\n",
    "        \"courier_id\",\n",
    "        \"city\",\n",
    "        \"accept_time\",\n",
    "        \"pickup_time\",\n",
    "        \"time_window_start\",\n",
    "        \"time_window_end\"])\n",
    ")\n",
    "\n",
    "print(\"After dropping duplicates and dropna on key columns:\", df_silver.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7769aa09-1a70-4b12-88a9-3494c9940c08",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The raw timestamp columns lack a year component, so I temporarily prefixed a fixed year to each time-related column. This makes the values compatible with Spark’s timestamp parsing functions. This prevents malformed timestamps from silently corrupting duration calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6662c50b-3010-41a6-a842-e18e70404112",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_silver = (\n",
    "    df_silver\n",
    "    .withColumn(\"accept_time_fixed\", F.concat(F.lit(\"2026-\"), F.col(\"accept_time\")))\n",
    "    .withColumn(\"pickup_time_fixed\", F.concat(F.lit(\"2026-\"), F.col(\"pickup_time\")))\n",
    "    .withColumn(\"window_start_fixed\", F.concat(F.lit(\"2026-\"), F.col(\"time_window_start\")))\n",
    "    .withColumn(\"window_end_fixed\", F.concat(F.lit(\"2026-\"), F.col(\"time_window_end\")))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5668afa5-f918-4973-9fe2-c7c99006a44a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Using try_to_timestamp, I converted the fixed timestamp strings into proper timestamp columns. Invalid or malformed values are automatically handled without breaking the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a504734-8b07-47ee-a80d-3200651ff63f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_silver = (\n",
    "    df_silver\n",
    "    .withColumn(\"accept_time\", F.expr(\"try_to_timestamp(accept_time_fixed, 'yyyy-MM-dd HH:mm:ss')\"))\n",
    "    .withColumn(\"pickup_time\", F.expr(\"try_to_timestamp(pickup_time_fixed, 'yyyy-MM-dd HH:mm:ss')\"))\n",
    "    .withColumn(\"time_window_start\", F.expr(\"try_to_timestamp(window_start_fixed, 'yyyy-MM-dd HH:mm:ss')\"))\n",
    "    .withColumn(\"time_window_end\", F.expr(\"try_to_timestamp(window_end_fixed, 'yyyy-MM-dd HH:mm:ss')\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b667d0dd-49eb-41b8-b48e-85ac378781da",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"accept_to_pickup_minutes\":215},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1769842233587}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df_silver.limit(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a6c075b-8dbb-4e6a-80db-9d6a85901c20",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "After conversion, I removed any rows where timestamp parsing failed. This ensures that all time-based calculations in the Silver layer are based on valid timestamps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e622932b-f3eb-4906-bd79-1361ac60818c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_silver = df_silver.dropna(\n",
    "    subset=[\"accept_time\", \"pickup_time\", \"time_window_start\", \"time_window_end\"]\n",
    ")\n",
    "\n",
    "print(\"After timestamp fix:\", df_silver.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d67fc25c-f5f4-4247-9cec-a9952117f701",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Once the timestamps were successfully converted, I dropped the temporary helper columns to keep the schema clean and focused on business-relevant fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4377340a-0d6a-4233-9387-c162fdf92a70",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_silver = df_silver.drop(\n",
    "    \"accept_time_fixed\",\n",
    "    \"pickup_time_fixed\",\n",
    "    \"window_start_fixed\",\n",
    "    \"window_end_fixed\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7639e4d2-245b-4cb8-ba52-cd343492a838",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Here, I calculated the time difference between order acceptance and pickup in minutes. This metric helps measure courier responsiveness and operational efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34ce3baf-173c-415f-88ba-bbfdba581032",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_silver = df_silver.withColumn(\n",
    "    \"accept_to_pickup_minutes\",\n",
    "    (F.col(\"pickup_time\").cast(\"long\") - F.col(\"accept_time\").cast(\"long\")) / 60\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "682f29f7-4c03-42f1-98ae-e3bedf2c7388",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "In this step, I computed how late a pickup occurred relative to the end of its allowed time window. Positive values indicate late pickups, while negative values indicate early or on-time pickups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6dde2c0-5857-40d8-9053-7e4770e007c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_silver = df_silver.withColumn(\n",
    "    \"pickup_delay_minutes\",\n",
    "    (F.col(\"pickup_time\").cast(\"long\") - F.col(\"time_window_end\").cast(\"long\")) / 60\n",
    ")\n",
    "print(\"After adding duration columns:\", df_silver.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf472646-2c08-41fb-9d95-782506c566ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Since missing GPS data often indicates operational or tracking issues, I created a binary flag to mark records where pickup latitude or longitude is missing. After adding the GPS flag, I verified the row count to ensure the transformation only added information and did not remove data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "280d94db-e51e-4551-986f-3416ea0560fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "df_silver = df_silver.withColumn(\n",
    "    \"missing_pickup_gps\",\n",
    "    F.when(\n",
    "        F.col(\"pickup_gps_lat\").isNull() | F.col(\"pickup_gps_lng\").isNull(),\n",
    "        1\n",
    "    ).otherwise(0)\n",
    ")\n",
    "print(\"After adding missing GPS flag:\", df_silver.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78c4e411-72d5-48b1-a077-7d13e5320b6f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "I created an initial risk label based on delayed pickups, long accept-to-pickup times,or missing GPS data. This preliminary label is designed to highlight potentially problematic deliveries.It helps in exploring the data distribution before refining the final risk thresholds. I checked the row count again to confirm that adding the risk label was a non-destructive transformation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c44d5f6c-d8e6-4c24-90ef-9889f5c80e6b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_silver = df_silver.withColumn(\n",
    "    \"high_risk_delivery\",\n",
    "    F.when(\n",
    "        (F.col(\"pickup_delay_minutes\") > 10) |\n",
    "        (F.col(\"accept_to_pickup_minutes\") > 30) |\n",
    "        (F.col(\"missing_pickup_gps\") == 1),\n",
    "        1\n",
    "    ).otherwise(0)\n",
    ")\n",
    "print(\"After adding high-risk label:\", df_silver.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9122559a-9dd3-4685-826a-d467ccaf4cc3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Here, I inspected the maximum accept-to-pickup time to understand the scale of outliers present in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3da8f6fc-625e-4084-841c-6ac4f55210cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "df_silver.select(\n",
    "    F.max(\"accept_to_pickup_minutes\").alias(\"max_accept_to_pickup_minutes\")\n",
    ").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41557c3a-d747-4164-bdc7-a9669e095333",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_silver.select(\n",
    "    \"order_id\",\n",
    "    \"accept_to_pickup_minutes\",\n",
    "    \"accept_time\",\n",
    "    \"pickup_time\"\n",
    ").orderBy(\n",
    "    F.col(\"accept_to_pickup_minutes\").desc()\n",
    ").show(10, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "104f05af-6c02-49dc-866e-b8a22b8c083e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "I calculated the median, P95, P99, and maximum values for accept-to-pickup duration to better understand the distribution and identify realistic thresholds for risk classification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1dd57d88-b0e9-47e7-a2c3-3e87d69fd338",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_silver.select(\n",
    "    F.expr(\"percentile_approx(accept_to_pickup_minutes, 0.50)\").alias(\"median\"),\n",
    "    F.expr(\"percentile_approx(accept_to_pickup_minutes, 0.95)\").alias(\"p95\"),\n",
    "    F.expr(\"percentile_approx(accept_to_pickup_minutes, 0.99)\").alias(\"p99\"),\n",
    "    F.max(\"accept_to_pickup_minutes\").alias(\"max\")\n",
    ").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44c4a801-ef46-464f-8f1e-3080d1f43f3e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "I displayed the records with the highest accept-to-pickup durations to visually verify whether these extreme values are plausible or represent data anomalies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7990dcfa-d060-493d-b3e4-ce2326345d71",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_silver.withColumn(\n",
    "    \"bucket\",\n",
    "    F.when(F.col(\"accept_to_pickup_minutes\") < 30, \"<30m\")\n",
    "     .when(F.col(\"accept_to_pickup_minutes\") < 60, \"30–60m\")\n",
    "     .when(F.col(\"accept_to_pickup_minutes\") < 120, \"1–2h\")\n",
    "     .when(F.col(\"accept_to_pickup_minutes\") < 180, \"2–3h\")\n",
    "     .when(F.col(\"accept_to_pickup_minutes\") < 360, \"3–6h\")\n",
    "     .when(F.col(\"accept_to_pickup_minutes\") < 1440, \"6–24h\")\n",
    "     .otherwise(\">1 day\")\n",
    ").groupBy(\"bucket\").count().orderBy(\"bucket\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4af781e8-51b8-4ec9-9e83-6ccb2fb087d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Based on the distribution analysis, I updated the high-risk definition to use a more realistic threshold for accept-to-pickup duration, while retaining the other risk indicators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7008537-35ea-4334-885e-49ecb24076fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_silver = df_silver.withColumn(\n",
    "    \"high_risk_delivery\",\n",
    "    F.when(\n",
    "        (F.col(\"accept_to_pickup_minutes\") > 180) |\n",
    "        (F.col(\"pickup_delay_minutes\") > 10) |\n",
    "        (F.col(\"missing_pickup_gps\") == 1),\n",
    "        1\n",
    "    ).otherwise(0)\n",
    ")\n",
    "\n",
    "df_silver.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a3fb2cf6-2167-4c8f-88dc-022d7fcc243d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "I grouped records by the high-risk flag to confirm that both high-risk and non-risk deliveries are reasonably represented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f36890d8-900d-4801-afaf-304016492a03",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_silver.groupBy(\"high_risk_delivery\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f7a94bd-284c-46da-b509-46dd6cfa51db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "To validate the logic qualitatively, I displayed a small sample of high-risk deliveries and reviewed the contributing factors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "692f116c-9ad7-4599-9a3f-139796ea7132",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_silver.filter(F.col(\"high_risk_delivery\") == 1) \\\n",
    "    .select(\"accept_to_pickup_minutes\", \"pickup_delay_minutes\", \"missing_pickup_gps\") \\\n",
    "    .show(10, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd691642-5169-4da0-a386-9dc662793ba8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Execution Plan\n",
    "\n",
    "By calling `df_silver.explain(mode=\"formatted\")`, I can inspect:\n",
    "- The **logical plan**: what transformations are requested\n",
    "- The **optimized plan**: how Spark simplifies and reorders operations\n",
    "- The **physical plan**: how the computation will actually be executed across the cluster\n",
    "\n",
    "This step helps verify that Spark is efficiently combining transformations and avoiding\n",
    "unnecessary scans or shuffles before persisting the Silver table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d6347f4-b9aa-46ac-82d7-871d8ceb4bdb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_silver.explain(mode=\"formatted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f4a617d-030b-43e6-95f7-29ad66bb2395",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "After validating the transformations and risk logic, I persisted the cleaned and enriched dataset as a Silver Delta table. This table serves as a trusted, analytics-ready source for downstream Gold aggregations and machine learning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4efcaba-1338-48e4-9734-05e0119eef91",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_silver.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .saveAsTable(\"capstone_project.logistics.silver_last_mile_deliveries\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7640075364938846,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "02_silver_transformations",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
