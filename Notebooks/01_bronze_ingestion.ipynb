{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cc0e0a24-2fe1-42ea-911f-df1d4598532b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Before reading any files, I first listed the contents of the raw data volume to confirm that the last-mile delivery parquet files are available and accessible. This helps ensure the ingestion path is correct before proceeding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8241c890-053e-4d9b-ac7f-38facf446246",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.ls(\"/Volumes/capstone_project/logistics/last_mile_raw\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "378dbf87-0adc-4f44-9e2e-a06b9eb3b4af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "In the below cell, I loaded the pickup data of 5 different cities from parquet files into a Spark DataFrame. Each city is read separately so that the data sources remain clear and traceable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "91cdc1da-ba65-40b9-b3f9-051b4ea7bcaf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_shanghai = spark.read.parquet(\n",
    "   \"/Volumes/capstone_project/logistics/last_mile_raw/pickup_sh-00000-of-00001-79fabe8088e723a2.parquet\")\n",
    "\n",
    "df_hangzhou = spark.read.parquet(\n",
    "    \"/Volumes/capstone_project/logistics/last_mile_raw/pickup_hz-00000-of-00001-2641abebfe50648a.parquet\")\n",
    "\n",
    "df_chongqing = spark.read.parquet(\n",
    "    \"/Volumes/capstone_project/logistics/last_mile_raw/pickup_cq-00000-of-00001-a172031e5392f9d3.parquet\")\t\n",
    "\n",
    "df_jilin = spark.read.parquet(\n",
    "    \"/Volumes/capstone_project/logistics/last_mile_raw/pickup_jl-00000-of-00001-9b430a56a935f284.parquet\")\n",
    "\n",
    "df_yantai = spark.read.parquet(\n",
    "    \"/Volumes/capstone_project/logistics/last_mile_raw/pickup_yt-00000-of-00001-6d21a4dccd28ee03.parquet\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c0229944-ea87-4a44-9cce-54a3c9b33f33",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Since the raw datasets do not contain a city column, I added one manually to each DataFrame. This preserves source context once the datasets are combined and enables city-level analysis later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c041ce1-a4c6-4a2d-a2dc-2bf7dd457050",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "\n",
    "df_shanghai = df_shanghai.withColumn(\"city\", lit(\"Shanghai\"))\n",
    "df_hangzhou = df_hangzhou.withColumn(\"city\", lit(\"Hangzhou\"))\n",
    "df_chongqing = df_chongqing.withColumn(\"city\", lit(\"Chongqing\"))\n",
    "df_jilin = df_jilin.withColumn(\"city\", lit(\"Jilin\"))\n",
    "df_yantai = df_yantai.withColumn(\"city\", lit(\"Yantai\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "084a5871-49e0-428f-aac8-dad4b5a19c97",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "After adding the city column, I merged all city-level DataFrames into a single unified DataFrame using unionByName to ensure correct column alignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46b1bb8a-c3cc-4fc7-a754-06471f3e13ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_all = (\n",
    "    df_shanghai\n",
    "    .unionByName(df_hangzhou)\n",
    "    .unionByName(df_chongqing)\n",
    "    .unionByName(df_jilin)\n",
    "    .unionByName(df_yantai)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "674e60c1-3461-4220-92e6-117d31490b29",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Before writing the data, I explicitly set the catalog and schema to ensure the table is created in the correct project namespace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dcc0e4c4-5682-4583-8c6e-dd8f0a787859",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "USE CATALOG capstone_project;\n",
    "USE SCHEMA logistics;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8a25bd3c-f52d-42e0-bbba-6f884e2cd8e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "In the below cell, I added an ingestion timestamp to track when the data entered the system. This metadata is useful for auditing and pipeline monitoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb123b24-ef87-4d99-b7ac-ed0ea8c1a872",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_timestamp\n",
    "\n",
    "df_bronze = (\n",
    "    df_all\n",
    "    .withColumn(\"ingestion_timestamp\", current_timestamp())\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bafddb1e-e88f-4ffc-8120-f0e3e0d1bf2b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Finally, I wrote the combined dataset to a Delta table in overwrite mode. This creates the Bronze layer, which stores raw ingested data with minimal transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8da85fe5-6108-4906-994e-0537504df0df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_bronze.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .saveAsTable(\"bronze_last_mile_deliveries\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5534574778556178,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "01_bronze_ingestion",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
